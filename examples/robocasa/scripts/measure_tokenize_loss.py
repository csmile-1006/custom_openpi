import os
import json
import logging
import dataclasses
import multiprocessing
import random

import numpy as np
import sentencepiece
import tyro
import tqdm_loggable.auto as tqdm

# JAX/OpenPI ê´€ë ¨
import openpi.training.config as _config
import openpi.training.data_loader as _data_loader
import openpi.transforms as _transforms
import openpi.shared.download as download

from openpi.models.fast_tokenizer import UniversalActionProcessor
from torch.utils.data import Subset

# ë©€í‹°í”„ë¡œì„¸ì‹± ì‹œì‘ ë°©ì‹ ê³ ì • (HPC/Slurm í™˜ê²½ ì¼ê´€ì„±)
multiprocessing.set_start_method('spawn', force=True)


@dataclasses.dataclass
class Args:
    config_name: str
    output_dir: str
    batch_size: int = 1
    num_workers: int = 16
    max_frames: int | None = None
    sample_ratio: float = 1.0
    seed: int = 42


def create_dataset(config: _config.TrainConfig, sample_ratio: float = 1.0, seed: int = 42) -> tuple[_config.DataConfig, _data_loader.Dataset]:
    data_config = config.data.create(config.assets_dirs, config.model)
    if data_config.repo_id is None:
        raise ValueError("Data config must have a repo_id")
    
    # ë‹¨ì¼ ë°ì´í„°ì…‹ë§Œ ì²˜ë¦¬
    dataset = _data_loader.create_dataset(data_config, config.model)
    
    # Transform ì ìš©
    final_dataset = _data_loader.TransformedDataset(
        dataset,
        [
            *data_config.repack_transforms.inputs,
            *data_config.data_transforms.inputs,
            _transforms.Normalize(
                data_config.norm_stats, 
                use_quantiles=data_config.use_quantile_norm,
                key_mapping=data_config.normalize_key_mapping
            ),
            *data_config.model_transforms.inputs,
        ],
    )
        
    # ìƒ˜í”Œë§ ì ìš© (sample_ratio < 1.0ì¼ ë•Œë§Œ)
    if sample_ratio < 1.0:
        n = len(final_dataset)
        sample_size = int(n * sample_ratio)
        rng = random.Random(seed)
        indices = rng.sample(range(n), sample_size)
        final_dataset = Subset(final_dataset, indices)
        logging.info(f"Sampled {sample_size} from {n} total samples (ratio: {sample_ratio})")
    
    return data_config, final_dataset


def measure_tokenize_detokenize_loss(args: Args):
    config = _config.get_config(args.config_name)
    data_config, dataset = create_dataset(config, sample_ratio=args.sample_ratio, seed=args.seed)

    num_frames = len(dataset)
    
    # shuffleì™€ num_batches ê²°ì •
    shuffle = False
    if args.sample_ratio < 1.0:
        shuffle = True  # ìƒ˜í”Œë§ ì‹œ shuffle
    
    if args.max_frames is not None and args.max_frames < num_frames:
        num_batches = args.max_frames // args.batch_size
        shuffle = True  # max_frames ì œí•œ ì‹œ shuffle
    else:
        num_batches = num_frames // args.batch_size
    
    data_loader = _data_loader.TorchDataLoader(
        dataset=dataset,
        local_batch_size=args.batch_size,
        num_workers=args.num_workers,
        shuffle=shuffle,
        num_batches=num_batches,
    )

    # DataLoaderImplë¡œ ê°ì‹¸ì„œ SFTBatch í˜•íƒœë¡œ ë³€í™˜
    class DataLoaderImpl:
        def __init__(self, data_config, torch_data_loader):
            self._data_config = data_config
            self._data_loader = torch_data_loader

        def data_config(self):
            return self._data_config

        def __iter__(self):
            for batch in self._data_loader:
                # SFT í˜•íƒœë¡œ ë³€í™˜
                from openpi.models import model as _model
                observation = _model.Observation.from_dict(batch)
                actions = batch["actions"]
                yield _model.SFTBatch(observation=observation, actions=actions)

    wrapped_data_loader = DataLoaderImpl(data_config, data_loader)

    # ë¡œê¹… ì •ë³´
    logging.info(
        f"Dataset: {num_frames} samples, batch_size={args.batch_size}, "
        f"num_batches={num_batches}, max_frames={args.max_frames}, shuffle={shuffle}"
    )

    # ----- Output ì¤€ë¹„ -----
    os.makedirs(args.output_dir, exist_ok=True)

    # Configì—ì„œ action dimensions ê°€ì ¸ì˜¤ê¸°
    action_horizon = config.model.action_horizon  # 16
    action_dim = config.model.action_dim  # 12

    # UniversalActionProcessorë¥¼ from_pretrainedë¡œ ì´ˆê¸°í™”
    fast_tok = UniversalActionProcessor.from_pretrained(
        "physical-intelligence/fast",
    )
    
    # PaliGemma tokenizer ë³„ë„ ì´ˆê¸°í™” (tokenizer.pyì™€ ë™ì¼í•œ ë°©ì‹)
    path = download.maybe_download("gs://big_vision/paligemma_tokenizer.model", gs={"token": "anon"})
    with path.open("rb") as f:
        paligemma_tokenizer = sentencepiece.SentencePieceProcessor(model_proto=f.read())
    
    fast_skip_tokens = 128  # FASTTokenizerì—ì„œ ì‚¬ìš©í•˜ë˜ ê°’
    
    # Loss ê³„ì‚°ìš© ë³€ìˆ˜ë“¤
    full_token_losses = []  # ì „ì²´ í† í°ìœ¼ë¡œ detokenizeí•œ ê²½ìš°ì˜ loss (ì „ì²´ í‰ê· )
    first_10_token_losses = []  # ì²˜ìŒ 10ê°œ í† í°ìœ¼ë¡œ detokenizeí•œ ê²½ìš°ì˜ loss (ì „ì²´ í‰ê· )
    
    # ì°¨ì›ë³„ loss ê³„ì‚°ìš© ë³€ìˆ˜ë“¤
    full_token_losses_per_dim = []  # ì „ì²´ í† í°: ê° ì°¨ì›ë³„ loss (N, 7)
    first_10_token_losses_per_dim = []  # ì²˜ìŒ 10ê°œ í† í°: ê° ì°¨ì›ë³„ loss (N, 7)
    
    # ë³µì›ë¥  ê³„ì‚°ì„ ìœ„í•œ ì›ë³¸ variance ê³„ì‚°ìš©
    original_actions_list = []  # ì›ë³¸ actionë“¤ (ë’¤ 7ê°œ ì°¨ì›ë§Œ)
    
    total_sequences = 0
    skipped_sequences = 0

    # ë©”ì¸ ì²˜ë¦¬ ë£¨í”„
    for batch_idx, sft_batch in enumerate(
        tqdm.tqdm(wrapped_data_loader, desc="Measuring tokenize->detokenize loss")
    ):
        obs = sft_batch.observation
        actions = sft_batch.actions  # ì›ë³¸ normalized actions (B, H, D)
        tokens = np.asarray(obs.tokenized_prompt)              # (B, L)
        token_mask = np.asarray(obs.tokenized_prompt_mask)     # (B, L)
        loss_mask = np.asarray(obs.token_loss_mask)            # (B, L); postfix(True)

        B = tokens.shape[0]

        for i in range(B):
            valid = token_mask[i].astype(bool)
            post = loss_mask[i].astype(bool)

            seq = tokens[i][valid]
            post_seq_mask = post[valid]

            post_idxs = np.where(post_seq_mask)[0]
            if post_idxs.size == 0:
                skipped_sequences += 1
                continue

            postfix_segment = seq[post_idxs]
            if postfix_segment.size == 0:
                skipped_sequences += 1
                continue

            # extract_actions ë°©ì‹ì„ ë”°ë¼ í…ìŠ¤íŠ¸ ë ˆë²¨ì—ì„œ ì²˜ë¦¬
            # 1. postfix segmentë¥¼ í…ìŠ¤íŠ¸ë¡œ ë””ì½”ë”©
            decoded_tokens = paligemma_tokenizer.decode(postfix_segment.tolist())
            
            # 2. "Action: " ì²´í¬ ë° ì¶”ì¶œ
            if "Action: " not in decoded_tokens:
                skipped_sequences += 1
                continue
            
            # 3. Action: ì´í›„ ë¶€ë¶„ì„ ì¶”ì¶œí•˜ê³  | ì´ì „ê¹Œì§€ë§Œ ê°€ì ¸ì˜¤ê¸°
            try:
                action_part = decoded_tokens.split("Action: ")[1].split("|")[0].strip()
            except (IndexError, AttributeError):
                skipped_sequences += 1
                continue
                
            if not action_part:
                skipped_sequences += 1
                continue
            
            # 4. í…ìŠ¤íŠ¸ë¥¼ ë‹¤ì‹œ í† í°í™”
            raw_action_tokens = np.array(paligemma_tokenizer.encode(action_part))
            if raw_action_tokens.size == 0:
                skipped_sequences += 1
                continue

            # ì›ë³¸ action chunk ê°€ì ¸ì˜¤ê¸° (B, H, D) -> (H, D)
            original_actions = actions[i]  # (16, 12)
            
            # 5. PaliGemma tokenì„ FAST tokenìœ¼ë¡œ ë³€í™˜ (extract_actionsì™€ ë™ì¼)
            fast_action_tokens = paligemma_tokenizer.vocab_size() - 1 - fast_skip_tokens - raw_action_tokens
            
            # 1. ì „ì²´ í† í°ìœ¼ë¡œ detokenize
            try:
                decoded_full = fast_tok.decode(
                    [fast_action_tokens.tolist()], 
                    time_horizon=action_horizon, 
                    action_dim=action_dim
                )[0]  # (16, 12)
                
                # Loss ê³„ì‚°: ë’¤ì˜ 7ê°œ ì°¨ì›ì— ëŒ€í•´ì„œë§Œ (dim 5~11)
                diff = original_actions[:, 5:] - decoded_full[:, 5:]  # (16, 7)
                loss_per_timestep_dim = diff ** 2  # (16, 7)
                
                # ì „ì²´ í‰ê·  loss (ê¸°ì¡´ ë°©ì‹)
                loss_full = np.mean(loss_per_timestep_dim)
                full_token_losses.append(loss_full)
                
                # ì°¨ì›ë³„ loss (timestepì— ëŒ€í•´ í‰ê· , ì°¨ì›ë³„ë¡œ ìœ ì§€)
                loss_per_dim = np.mean(loss_per_timestep_dim, axis=0)  # (7,)
                full_token_losses_per_dim.append(loss_per_dim)
                
                # ì›ë³¸ action ì €ì¥ (ë³µì›ë¥  ê³„ì‚°ìš©)
                original_actions_list.append(original_actions[:, 5:].flatten())
                
            except Exception as e:
                logging.warning(f"Failed to decode full tokens for batch {batch_idx}, sample {i}: {e}")
                logging.warning(f"Tokens: {fast_action_tokens.tolist()}")
                skipped_sequences += 1
                continue
            
            # 2. ì²˜ìŒ 10ê°œ í† í°ë§Œìœ¼ë¡œ detokenize (ë˜ëŠ” ê°€ëŠ¥í•œ ë§Œí¼ë§Œ)
            if raw_action_tokens.size > 0:  # í† í°ì´ ìˆìœ¼ë©´ ì²˜ë¦¬
                try:
                    # ìµœëŒ€ 10ê°œê¹Œì§€ë§Œ ì‚¬ìš© (í† í°ì´ ë¶€ì¡±í•˜ë©´ ê°€ëŠ¥í•œ ë§Œí¼ë§Œ)
                    num_tokens_to_use = min(10, raw_action_tokens.size)
                    first_tokens = fast_action_tokens[:num_tokens_to_use]
                    
                    # FAST tokenizerê°€ ìë™ìœ¼ë¡œ íŒ¨ë”© ì²˜ë¦¬í•¨
                    decoded_partial = fast_tok.decode(
                        [first_tokens.tolist()], 
                        time_horizon=action_horizon, 
                        action_dim=action_dim
                    )[0]  # (16, 12)
                    
                    # Loss ê³„ì‚°: ë’¤ì˜ 7ê°œ ì°¨ì›ì— ëŒ€í•´ì„œë§Œ (dim 5~11)
                    diff_partial = original_actions[:, 5:] - decoded_partial[:, 5:]  # (16, 7)
                    loss_per_timestep_dim_partial = diff_partial ** 2  # (16, 7)
                    
                    # ì „ì²´ í‰ê·  loss (ê¸°ì¡´ ë°©ì‹)
                    loss_partial = np.mean(loss_per_timestep_dim_partial)
                    first_10_token_losses.append(loss_partial)
                    
                    # ì°¨ì›ë³„ loss (timestepì— ëŒ€í•´ í‰ê· , ì°¨ì›ë³„ë¡œ ìœ ì§€)
                    loss_per_dim_partial = np.mean(loss_per_timestep_dim_partial, axis=0)  # (7,)
                    first_10_token_losses_per_dim.append(loss_per_dim_partial)
                    
                except Exception as e:
                    logging.warning(f"Failed to decode partial tokens for batch {batch_idx}, sample {i}: {e}")
                    logging.warning(f"Tokens: {first_tokens.tolist()}")
                    # ì´ ê²½ìš°ëŠ” ì „ì²´ ì‹œí€€ìŠ¤ ì¹´ìš´íŠ¸ì— ì˜í–¥ ì£¼ì§€ ì•ŠìŒ (ì´ë¯¸ full decodeëŠ” ì„±ê³µí–ˆìœ¼ë¯€ë¡œ)
                    continue
            
            total_sequences += 1

    # ë³µì›ë¥  ê³„ì‚° (ì›ë³¸ ë°ì´í„°ì˜ variance ëŒ€ë¹„ MSE loss)
    if original_actions_list:
        all_original_actions = np.concatenate(original_actions_list, axis=0)  # (N*16*7,)
        original_variance = np.var(all_original_actions)
        
        # ì°¨ì›ë³„ ì›ë³¸ variance ê³„ì‚°
        all_original_reshaped = np.array(original_actions_list)  # (N, 16*7)
        all_original_per_dim = all_original_reshaped.reshape(-1, 16, 7)  # (N, 16, 7)
        original_variance_per_dim = np.var(all_original_per_dim, axis=(0, 1))  # (7,) - ê° ì°¨ì›ë³„ variance
        
        # ì „ì²´ ë³µì›ë¥ 
        full_recovery_rate = (1 - np.mean(full_token_losses) / original_variance) * 100 if original_variance > 0 else 0.0
        partial_recovery_rate = (1 - np.mean(first_10_token_losses) / original_variance) * 100 if original_variance > 0 and first_10_token_losses else 0.0
        
        # ì°¨ì›ë³„ ë³µì›ë¥  ê³„ì‚°
        if full_token_losses_per_dim and len(full_token_losses_per_dim) > 0:
            full_losses_per_dim_array = np.array(full_token_losses_per_dim)  # (N, 7)
            full_mean_loss_per_dim = full_losses_per_dim_array.mean(axis=0)  # (7,)
            full_recovery_rate_per_dim = []
            for dim_idx in range(7):
                if original_variance_per_dim[dim_idx] > 0:
                    recovery_rate = (1 - full_mean_loss_per_dim[dim_idx] / original_variance_per_dim[dim_idx]) * 100
                    full_recovery_rate_per_dim.append(recovery_rate)
                else:
                    full_recovery_rate_per_dim.append(0.0)
        else:
            full_recovery_rate_per_dim = [0.0] * 7
            
        if first_10_token_losses_per_dim and len(first_10_token_losses_per_dim) > 0:
            partial_losses_per_dim_array = np.array(first_10_token_losses_per_dim)  # (N, 7)
            partial_mean_loss_per_dim = partial_losses_per_dim_array.mean(axis=0)  # (7,)
            partial_recovery_rate_per_dim = []
            for dim_idx in range(7):
                if original_variance_per_dim[dim_idx] > 0:
                    recovery_rate = (1 - partial_mean_loss_per_dim[dim_idx] / original_variance_per_dim[dim_idx]) * 100
                    partial_recovery_rate_per_dim.append(recovery_rate)
                else:
                    partial_recovery_rate_per_dim.append(0.0)
        else:
            partial_recovery_rate_per_dim = [0.0] * 7
        
        # ì°¨ì›ë³„ í†µê³„ ê³„ì‚°
        if full_token_losses_per_dim:
            full_losses_per_dim_array = np.array(full_token_losses_per_dim)  # (N, 7)
            full_losses_per_dim_stats = {
                "mean": full_losses_per_dim_array.mean(axis=0).tolist(),  # ê° ì°¨ì›ë³„ í‰ê· 
                "std": full_losses_per_dim_array.std(axis=0).tolist(),    # ê° ì°¨ì›ë³„ í‘œì¤€í¸ì°¨
                "min": full_losses_per_dim_array.min(axis=0).tolist(),    # ê° ì°¨ì›ë³„ ìµœì†Œê°’
                "max": full_losses_per_dim_array.max(axis=0).tolist(),    # ê° ì°¨ì›ë³„ ìµœëŒ€ê°’
            }
        else:
            full_losses_per_dim_stats = {"mean": [0.0]*7, "std": [0.0]*7, "min": [0.0]*7, "max": [0.0]*7}
            
        if first_10_token_losses_per_dim:
            partial_losses_per_dim_array = np.array(first_10_token_losses_per_dim)  # (N, 7)
            partial_losses_per_dim_stats = {
                "mean": partial_losses_per_dim_array.mean(axis=0).tolist(),
                "std": partial_losses_per_dim_array.std(axis=0).tolist(),
                "min": partial_losses_per_dim_array.min(axis=0).tolist(),
                "max": partial_losses_per_dim_array.max(axis=0).tolist(),
            }
        else:
            partial_losses_per_dim_stats = {"mean": [0.0]*7, "std": [0.0]*7, "min": [0.0]*7, "max": [0.0]*7}
    else:
        original_variance = 0.0
        full_recovery_rate = 0.0
        partial_recovery_rate = 0.0
        full_recovery_rate_per_dim = [0.0] * 7
        partial_recovery_rate_per_dim = [0.0] * 7
        original_variance_per_dim = [0.0] * 7
        full_losses_per_dim_stats = {"mean": [0.0]*7, "std": [0.0]*7, "min": [0.0]*7, "max": [0.0]*7}
        partial_losses_per_dim_stats = {"mean": [0.0]*7, "std": [0.0]*7, "min": [0.0]*7, "max": [0.0]*7}

    # ê²°ê³¼ ì €ì¥ ë° ì¶œë ¥
    results = {
        "total_sequences": total_sequences,
        "skipped_sequences": skipped_sequences,
        "valid_full_sequences": len(full_token_losses),
        "valid_partial_sequences": len(first_10_token_losses),
        "full_token_loss": {
            "mean": float(np.mean(full_token_losses)) if full_token_losses else 0.0,
            "std": float(np.std(full_token_losses)) if full_token_losses else 0.0,
            "min": float(np.min(full_token_losses)) if full_token_losses else 0.0,
            "max": float(np.max(full_token_losses)) if full_token_losses else 0.0,
        },
        "first_10_token_loss": {
            "mean": float(np.mean(first_10_token_losses)) if first_10_token_losses else 0.0,
            "std": float(np.std(first_10_token_losses)) if first_10_token_losses else 0.0,
            "min": float(np.min(first_10_token_losses)) if first_10_token_losses else 0.0,
            "max": float(np.max(first_10_token_losses)) if first_10_token_losses else 0.0,
        },
        "recovery_rates": {
            "full_token_recovery_rate_percent": float(full_recovery_rate),
            "first_10_token_recovery_rate_percent": float(partial_recovery_rate),
            "original_variance": float(original_variance),
            "full_token_recovery_rate_per_dim_percent": [float(x) for x in full_recovery_rate_per_dim],
            "first_10_token_recovery_rate_per_dim_percent": [float(x) for x in partial_recovery_rate_per_dim],
            "original_variance_per_dim": [float(x) for x in original_variance_per_dim],
        },
        "config": {
            "action_horizon": action_horizon,
            "action_dim": action_dim,
            "loss_dimensions": "5:12 (7 dims)",  # ë’¤ì˜ 7ê°œ ì°¨ì›
        },
        "full_losses_per_dim_stats": full_losses_per_dim_stats,
        "first_10_losses_per_dim_stats": partial_losses_per_dim_stats,
    }

    # ê²°ê³¼ ì €ì¥
    results_path = os.path.join(args.output_dir, "tokenize_detokenize_loss_results.json")
    with open(results_path, "w") as f:
        json.dump(results, f, indent=2)
    
    logging.info(f"Saved results to {os.path.abspath(results_path)}")
    
    # ê²°ê³¼ ì¶œë ¥
    logging.info("=== Tokenize -> Detokenize Loss Results ===")
    logging.info(f"Total sequences: {total_sequences}, Skipped: {skipped_sequences}")
    logging.info(f"Valid full sequences: {len(full_token_losses)}, Valid partial sequences: {len(first_10_token_losses)}")
    logging.info(f"Original data variance: {results['recovery_rates']['original_variance']:.6f}")
    logging.info(f"Full token loss (meanÂ±std): {results['full_token_loss']['mean']:.6f}Â±{results['full_token_loss']['std']:.6f}")
    logging.info(f"First 10 token loss (meanÂ±std): {results['first_10_token_loss']['mean']:.6f}Â±{results['first_10_token_loss']['std']:.6f}")
    logging.info(f"ğŸ¯ Full token recovery rate: {results['recovery_rates']['full_token_recovery_rate_percent']:.2f}%")
    logging.info(f"ğŸ¯ First 10 token recovery rate: {results['recovery_rates']['first_10_token_recovery_rate_percent']:.2f}%")
    
    # ì°¨ì›ë³„ loss ì¶œë ¥
    logging.info("=== Per-Dimension Loss Analysis ===")
    if full_losses_per_dim_stats["mean"]:
        logging.info("Full token losses per dimension (5~11):")
        for dim_idx, (mean_loss, std_loss) in enumerate(zip(full_losses_per_dim_stats["mean"], full_losses_per_dim_stats["std"])):
            logging.info(f"  Dim {dim_idx+5}: {mean_loss:.6f}Â±{std_loss:.6f}")
    
    if partial_losses_per_dim_stats["mean"]:
        logging.info("First 10 token losses per dimension (5~11):")
        for dim_idx, (mean_loss, std_loss) in enumerate(zip(partial_losses_per_dim_stats["mean"], partial_losses_per_dim_stats["std"])):
            logging.info(f"  Dim {dim_idx+5}: {mean_loss:.6f}Â±{std_loss:.6f}")
    
    # ì°¨ì›ë³„ ë³µì›ë¥  ì¶œë ¥
    logging.info("=== Per-Dimension Recovery Rates ===")
    if full_recovery_rate_per_dim:
        logging.info("Full token recovery rates per dimension (5~11):")
        for dim_idx, recovery_rate in enumerate(full_recovery_rate_per_dim):
            logging.info(f"  Dim {dim_idx+5}: {recovery_rate:.2f}%")
    
    if partial_recovery_rate_per_dim:
        logging.info("First 10 token recovery rates per dimension (5~11):")
        for dim_idx, recovery_rate in enumerate(partial_recovery_rate_per_dim):
            logging.info(f"  Dim {dim_idx+5}: {recovery_rate:.2f}%")
    
    # ìƒì„¸í•œ ë¶„í¬ ì €ì¥
    detailed_results = {
        "full_token_losses": [float(x) for x in full_token_losses],
        "first_10_token_losses": [float(x) for x in first_10_token_losses],
        "full_token_losses_per_dim": [x.tolist() for x in full_token_losses_per_dim],  # ì°¨ì›ë³„ loss
        "first_10_token_losses_per_dim": [x.tolist() for x in first_10_token_losses_per_dim],  # ì°¨ì›ë³„ loss
    }
    
    detailed_path = os.path.join(args.output_dir, "tokenize_detokenize_loss_detailed.json")
    with open(detailed_path, "w") as f:
        json.dump(detailed_results, f)
    
    logging.info(f"Saved detailed results to {os.path.abspath(detailed_path)}")


def main(args: Args):
    logging.basicConfig(level=logging.INFO, force=True)
    logging.info("Starting tokenize->detokenize loss measurement")
    measure_tokenize_detokenize_loss(args)


if __name__ == "__main__":
    args = tyro.cli(Args)
    main(args) 